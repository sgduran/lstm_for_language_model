{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import pprint\n",
    "import copy\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.backend as K\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Input, LSTM, Reshape, Lambda, RepeatVector\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sentences_with_eos_and_unk.pkl', 'rb') as f:\n",
    "    sentences, counter = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sentences_train_test.pkl', 'rb') as f:\n",
    "    sentences_train, sentences_test, counter = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7274 unique words in our dataset.\n",
      "There are 3380789 total words in our dataset.\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(counter.keys())\n",
    "vocab_size = len(vocab)\n",
    "print('There are %d unique words in our dataset.' % vocab_size)\n",
    "num_of_words = sum(counter.values())\n",
    "print('There are %d total words in our dataset.' % num_of_words)\n",
    "\n",
    "# Hash table for words to indices and viceversa\n",
    "# I don't use index 0 because it's used for padding the sequences\n",
    "word_to_ix = { w:i+1 for i,w in enumerate(vocab) }\n",
    "word_to_ix['<blank>'] = 0\n",
    "ix_to_word = { i+1:w for i,w in enumerate(vocab) }\n",
    "ix_to_word[0] = ''\n",
    "\n",
    "# Get frequency of words\n",
    "word_freq = np.zeros(vocab_size+1)\n",
    "norm_constant = num_of_words\n",
    "for i,key in enumerate(vocab):\n",
    "    word_freq[i+1] = counter[key] / norm_constant\n",
    "    \n",
    "# Get frequency of first words\n",
    "first_word_freq = np.zeros(vocab_size+1)\n",
    "norm_constant = len(sentences)\n",
    "for i,sentence in enumerate(sentences):\n",
    "    first_word_freq[ word_to_ix[sentence.lower().split()[0]] ] += 1\n",
    "first_word_freq /= len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_model = keras.models.load_model(\"trained_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = pickle.load(open('./trained_model_4_epochs/trainHistoryDict_4epochs.pkl', \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2830f54af48>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#print(f\"loss at epoch 1: {history['loss'][0]}\")\n",
    "#print(f\"loss at epoch 5: {history['loss'][2]}\")\n",
    "#print(f\"loss at epoch 10: {history['loss'][3]}\")\n",
    "plt.plot(history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of dimensions for the hidden state of each LSTM cell.\n",
    "n_a = 128\n",
    "n_values = len(vocab)+1 # number of words and the blank space\n",
    "Tx = 30 # max len of sequences\n",
    "reshaper = Reshape((1, n_values))                  # Used in Step 2.B of model(), below\n",
    "LSTM_cell = LSTM(n_a, return_state = True)         # Used in Step 2.C\n",
    "densor = Dense(n_values, activation='softmax')     # Used in Step 2.D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treat inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_indices(X, word_to_ix, max_len):\n",
    "    \"\"\"\n",
    "    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.\n",
    "    The output shape should be such that it can be given to `Embedding()` (described in Figure 4). \n",
    "    \n",
    "    Arguments:\n",
    "    X -- array of sentences (strings), of shape (m, 1)\n",
    "    word_to_index -- a dictionary containing the each word mapped to its index\n",
    "    max_len -- maximum number of words in a sentence. You can assume every sentence in X is no longer than this. \n",
    "    \n",
    "    Returns:\n",
    "    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[0]                                   # number of training examples\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Initialize X_indices as a numpy matrix of zeros and the correct shape (≈ 1 line)\n",
    "    X_indices = np.zeros( (m, max_len) )\n",
    "    \n",
    "    for i in range(m):                               # loop over training examples\n",
    "        \n",
    "        # Convert the ith training sentence in lower case and split is into words. You should get a list of words.\n",
    "        sentence_words = X[i].lower().split()\n",
    "        # We truncate sentence_words at max_len\n",
    "        sentence_words = sentence_words[:max_len]\n",
    "        \n",
    "        # Initialize j to 0\n",
    "        j = 0\n",
    "        \n",
    "        # Loop over the words of sentence_words\n",
    "\n",
    "        for w in sentence_words:\n",
    "            # if w exists in the word_to_index dictionary\n",
    "            if w in word_to_ix:\n",
    "                # Set the (i,j)th entry of X_indices to the index of the correct word.\n",
    "                X_indices[i, j] = word_to_ix[w]\n",
    "                # Increment j to j + 1\n",
    "                j += 1\n",
    "            \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return X_indices\n",
    "\n",
    "def convert_to_one_hot(X_indices, C):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)\n",
    "    C -- size of one-hot vectors -1 place for 1 word in vocabulary, plus blank space-\n",
    "    \n",
    "    Returns:\n",
    "    X_oh -- array of one-hot vectors corresponding to words in the sentences from X, of shape (m, max_len, C)\n",
    "    \"\"\"\n",
    "    X_oh = np.eye(C)[X_indices]\n",
    "    return X_oh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions over the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4528"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Put the clips part  part and part of the loom knoop hardness test value   channel B part  the loom HB2   channel B part and the No 2 starboard igniter lead part with the bolt part and the spacers part and part in position on the oil tube part and loosely install the nut part  <eos>',\n",
       " 'Put the clips part  part  part and part of the air tube part  the fuel tube part  the fuel tube part and the loom database part with the spacers part  part and part and the bolt part in position on the nut part attached to the bracket part  and loosely install the bolt part  <eos>',\n",
       " 'Make sure that the ignition lever part of the door damper part and the emergency opening actuator part is in the ARMED position  <eos>',\n",
       " 'You can use these pressurization sources as an alternative  or   <eos>',\n",
       " 'Disconnect the electrical connectors from the 115VAC EPDC2 temporary revision 1422XZ and the 230VAC EPDC2 temporary revision 1424XZ in module 2H   <eos>',\n",
       " 'Alodine 1500 mil <unk> <unk> <eos>',\n",
       " 'The transducer part <eos>',\n",
       " 'Remove the nuts part from the bolts part  <eos>',\n",
       " 'For the steps that follow the POLISHING KIT   MICRO MESH is necessary  <eos>',\n",
       " 'Loosen the nuts part of all the support struts part  <eos>',\n",
       " 'The Yellow left outboard aileron servocontrol  <eos>',\n",
       " 'The applicable lavatory occupied signs are removed   <eos>',\n",
       " 'Put the seals part in position on the left ottoman section part  the right ottoman section part and the upper coat stowage section part  <eos>',\n",
       " 'In the Status column wait for the status of the related content inventory list to show Load Complete   <eos>',\n",
       " 'Remove the nut part and the bolt part attached to the tube part  <eos>',\n",
       " 'Make sure that flap torque shaft assembly 5027CV part is removed   <eos>',\n",
       " 'It is possible to restore the avionics server function cabinet with the group 1 and amperes per square foot software components    only and then dispatch the aircraft under Master Minimum Equipment List MMEL condition <eos>',\n",
       " 'Bulk cargo compartment  <eos>',\n",
       " 'Re prime the center tank pumps   <eos>',\n",
       " 'If there is galling accept the upper interservices fairing  <eos>']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_test[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It is possible to restore the avionics server function cabinet with the group 1 and amperes per square foot software components    only and then dispatch the aircraft under Master Minimum Equipment List MMEL condition <eos>'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose sentence to predict, from validation set\n",
    "n = 16\n",
    "sentences_test[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3884. 5265. 6697. 5670. 6627. 1402. 5986. 3287. 1714. 7140. 6627. 3453.\n",
      "    62. 1245. 1236. 5105. 6267. 3140. 6174. 2037. 4866. 1245. 6631. 2525.\n",
      "  6627. 1171. 6871. 4495. 4587.    0.]] \n",
      " is possible to restore the avionics server function cabinet with the group 1 and amperes per square foot software components only and then dispatch the aircraft under master minimum equipment list mmel condition <eos>\n"
     ]
    }
   ],
   "source": [
    "# Build vectors needed to predict\n",
    "a0 = np.zeros((1, n_a))\n",
    "c0 = np.zeros((1, n_a))\n",
    "X_oh = np.zeros((1, Tx, len(vocab)+1))\n",
    "x_indices = sentences_to_indices(np.asarray(sentences_test[n:n+1]), word_to_ix, Tx)\n",
    "X_oh[0,:,:] = convert_to_one_hot(x_indices.astype(int), C = len(vocab)+1)\n",
    "\n",
    "# Build and print expected prediction (y_true value)\n",
    "y_indices = np.zeros(np.shape(x_indices))\n",
    "y_indices[:,0:Tx-1] = x_indices[:,1:Tx]\n",
    "print(y_indices, '\\n', ' '.join(sentences_test[n:n+1][0].lower().split()[1:]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3884 4725 6697 6627 6627 1275 2022 3287 1714  991 6627 6174  991  991\n",
      "  6627  991 6724 6950  991 2036 6625  991 6627 1301 3884 1171 3884 6627\n",
      "   991  991]] \n",
      " Is necessary to the the applicable compartment function cabinet <eos> the software <eos> <eos> the <eos> torque value <eos> component that <eos> the are is aircraft is the <eos> <eos>\n"
     ]
    }
   ],
   "source": [
    "# Obtain prediction\n",
    "pred = reconstructed_model.predict([X_oh, a0, c0])\n",
    "pred_indices = np.argmax(pred, axis=-1)\n",
    "\n",
    "# Build annd print sentence from prediction\n",
    "sample_sentence_words = [ix_to_word[int(index)] for index in list(pred_indices)]\n",
    "sample_sentence = ' '.join(sample_sentence_words)\n",
    "sample_sentence = sample_sentence[0].upper() + sample_sentence[1:]\n",
    "\n",
    "print(pred_indices.T, '\\n', sample_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Model 1: does not use probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define a model that uses each step prediction as the input for next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_inference_model1(LSTM_cell, densor, Ty=30):\n",
    "    \"\"\"\n",
    "    Uses the trained \"LSTM_cell\" and \"densor\" from model() to generate a sequence of values.\n",
    "    \n",
    "    Arguments:\n",
    "    LSTM_cell -- the trained \"LSTM_cell\" from model(), Keras layer object\n",
    "    densor -- the trained \"densor\" from model(), Keras layer object\n",
    "    Ty -- integer, number of time steps to generate\n",
    "    \n",
    "    Returns:\n",
    "    inference_model -- Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the shape of input values\n",
    "    n_values = densor.units\n",
    "    # Get the number of the hidden state vector\n",
    "    n_a = LSTM_cell.units\n",
    "    \n",
    "    # Define the input of your model with a shape \n",
    "    x0 = Input(shape=(1, n_values))\n",
    "    \n",
    "    # Define s0, initial hidden state for the decoder LSTM\n",
    "    a0 = Input(shape=(n_a,), name='a0')\n",
    "    c0 = Input(shape=(n_a,), name='c0')\n",
    "    a = a0\n",
    "    c = c0\n",
    "    x = x0\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    # Step 1: Create an empty list of \"outputs\" to later store your predicted values (≈1 line)\n",
    "    outputs = []\n",
    "    \n",
    "    # Step 2: Loop over Ty and generate a value at every time step\n",
    "    for t in range(Ty):\n",
    "        # Step 2.A: Perform one step of LSTM_cell. Use \"x\", not \"x0\" (≈1 line)\n",
    "        a, _, c = LSTM_cell(x, initial_state=[a,c])\n",
    "        \n",
    "        # Step 2.B: Apply Dense layer to the hidden state output of the LSTM_cell (≈1 line)\n",
    "        out = densor(a)\n",
    "        # Step 2.C: Append the prediction \"out\" to \"outputs\". out.shape = (None, n_values) (≈1 line)\n",
    "        outputs.append(out)\n",
    " \n",
    "        # Step 2.D: \n",
    "        # Select the next value according to \"out\",\n",
    "        # Set \"x\" to be the one-hot representation of the selected value\n",
    "        # See instructions above.\n",
    "        x = tf.math.argmax(out, axis=-1)\n",
    "        x = tf.one_hot(x,n_values)\n",
    "        # Step 2.E: \n",
    "        # Use RepeatVector(1) to convert x into a tensor with shape=(None, 1, n_values)\n",
    "        x = RepeatVector(1)(x)\n",
    "        \n",
    "    # Step 3: Create model instance with the correct \"inputs\" and \"outputs\" (≈1 line)\n",
    "    inference_model1 = Model(inputs=[x0,a0,c0], outputs=outputs)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return inference_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_model1 = sentence_inference_model1(LSTM_cell, densor, Ty = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful commands for transfer learning\n",
    "#reconstructed_model.layers[6]\n",
    "#LSTM_cell.set_weights(reconstructed_model.layers[6].get_weights())\n",
    "#densor.set_weights(reconstructed_model.layers[-1].get_weights())\n",
    "#inference_model1.load_weights(\"trained_weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_model1.set_weights(reconstructed_model.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use previous model to sample ('predict') a sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_initializer = np.zeros((1, 1, n_values))\n",
    "a_initializer = np.zeros((1, n_a))\n",
    "c_initializer = np.zeros((1, n_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_sample1(inference_model1, x_initializer = x_initializer, a_initializer = a_initializer, \n",
    "                       c_initializer = c_initializer):\n",
    "    \"\"\"\n",
    "    Predicts the next value of values using the inference model.\n",
    "    \n",
    "    Arguments:\n",
    "    inference_model -- Keras model instance for inference time\n",
    "    x_initializer -- numpy array of shape (1, 1, n_values), one-hot vector initializing the values generation\n",
    "    a_initializer -- numpy array of shape (1, n_a), initializing the hidden state of the LSTM_cell\n",
    "    c_initializer -- numpy array of shape (1, n_a), initializing the cell state of the LSTM_cel\n",
    "    \n",
    "    Returns:\n",
    "    results -- numpy-array of shape (Ty, 90), matrix of one-hot vectors representing the values generated\n",
    "    indices -- numpy-array of shape (Ty, 1), matrix of indices representing the values generated\n",
    "    \"\"\"\n",
    "    \n",
    "    n_values = x_initializer.shape[2]\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Step 1: Use your inference model to predict an output sequence given x_initializer, a_initializer and c_initializer.\n",
    "    pred = inference_model1.predict([x_initializer, a_initializer, c_initializer])\n",
    "    # Step 2: Convert \"pred\" into an np.array() of indices with the maximum probabilities\n",
    "    indices = np.argmax(pred, axis=-1)\n",
    "    # Step 3: Convert indices to one-hot vectors, the shape of the results should be (Ty, n_values)\n",
    "    results = to_categorical(indices, num_classes=x_initializer.shape[2])\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return results, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The applicable access panels and the door part <eos>                     \n"
     ]
    }
   ],
   "source": [
    "results, indices = predict_and_sample1(inference_model1, x_initializer, a_initializer, c_initializer)\n",
    "\n",
    "#print(\"np.argmax(results[12]) =\", np.argmax(results[0]))\n",
    "#print(\"np.argmax(results[17]) =\", np.argmax(results[17]))\n",
    "#print(\"list(indices[12:18]) =\", list(indices[12:18]))\n",
    "\n",
    "# Transform the result to an actual sentence\n",
    "sample_sentence_words = [ix_to_word[int(index)] for index in list(indices)]\n",
    "sample_sentence = ' '.join(sample_sentence_words)\n",
    "sample_sentence = sample_sentence[0].upper() + sample_sentence[1:]\n",
    "print(sample_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Model 2: choose at random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define a model that uses each step prediction as the input for next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_inference_model2(LSTM_cell, densor, Ty=30):\n",
    "    \"\"\"\n",
    "    Uses the trained \"LSTM_cell\" and \"densor\" from model() to generate a sequence of values.\n",
    "    \n",
    "    Arguments:\n",
    "    LSTM_cell -- the trained \"LSTM_cell\" from model(), Keras layer object\n",
    "    densor -- the trained \"densor\" from model(), Keras layer object\n",
    "    Ty -- integer, number of time steps to generate\n",
    "    \n",
    "    Returns:\n",
    "    inference_model -- Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the shape of input values\n",
    "    n_values = densor.units\n",
    "    # Get the number of the hidden state vector\n",
    "    n_a = LSTM_cell.units\n",
    "    \n",
    "    # Define the input of your model with a shape \n",
    "    x0 = Input(shape=(1, n_values))\n",
    "    \n",
    "    # Define s0, initial hidden state for the decoder LSTM\n",
    "    a0 = Input(shape=(n_a,), name='a0')\n",
    "    c0 = Input(shape=(n_a,), name='c0')\n",
    "    a = a0\n",
    "    c = c0\n",
    "    x = x0\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    # Step 1: Create an empty list of \"outputs\" to later store your predicted values (≈1 line)\n",
    "    outputs = []\n",
    "    \n",
    "    # Step 2: Loop over Ty and generate a value at every time step\n",
    "    for t in range(Ty):\n",
    "        # Step 2.A: Perform one step of LSTM_cell. Use \"x\", not \"x0\" (≈1 line)\n",
    "        a, _, c = LSTM_cell(x, initial_state=[a,c])\n",
    "        \n",
    "        # Step 2.B: Apply Dense layer to the hidden state output of the LSTM_cell (≈1 line)\n",
    "        out = densor(a)\n",
    "        outputs.append(out)\n",
    "        # Step 2.C: Append the prediction \"out\" to \"outputs\". out.shape = (None, n_values) (≈1 line)\n",
    " \n",
    "        # Step 2.D: \n",
    "        # Select the next value according to \"out\",\n",
    "        # Set \"x\" to be the one-hot representation of the selected value\n",
    "        # See instructions above.\n",
    "        \n",
    "        x = tf.math.argmax(out,axis=-1)        \n",
    "        x = tf.one_hot(x,n_values)\n",
    "        \n",
    "        # Step 2.E: \n",
    "        # Use RepeatVector(1) to convert x into a tensor with shape=(None, 1, n_values)\n",
    "        x = RepeatVector(1)(x)\n",
    "        \n",
    "    # Step 3: Create model instance with the correct \"inputs\" and \"outputs\" (≈1 line)\n",
    "    inference_model2 = Model(inputs=[x0,a0,c0], outputs=outputs)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return inference_model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_model2 = sentence_inference_model2(LSTM_cell, densor, Ty = 30)\n",
    "inference_model2.set_weights(reconstructed_model.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use previous model to sample ('predict') a sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_initializer = np.zeros((1, 1, n_values))\n",
    "a_initializer = np.zeros((1, n_a))\n",
    "c_initializer = np.zeros((1, n_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_sample2(inference_model2, x_initializer = x_initializer, a_initializer = a_initializer, \n",
    "                       c_initializer = c_initializer):\n",
    "    \"\"\"\n",
    "    Predicts the next value of values using the inference model.\n",
    "    \n",
    "    Arguments:\n",
    "    inference_model -- Keras model instance for inference time\n",
    "    x_initializer -- numpy array of shape (1, 1, n_values), one-hot vector initializing the values generation\n",
    "    a_initializer -- numpy array of shape (1, n_a), initializing the hidden state of the LSTM_cell\n",
    "    c_initializer -- numpy array of shape (1, n_a), initializing the cell state of the LSTM_cel\n",
    "    \n",
    "    Returns:\n",
    "    results -- numpy-array of shape (Ty, 90), matrix of one-hot vectors representing the values generated\n",
    "    indices -- numpy-array of shape (Ty, 1), matrix of indices representing the values generated\n",
    "    \"\"\"\n",
    "    \n",
    "    n_values = x_initializer.shape[2]\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Step 1: Use your inference model to predict an output sequence given x_initializer, a_initializer and c_initializer.\n",
    "    pred = inference_model2.predict([x_initializer, a_initializer, c_initializer])\n",
    "    Ty = np.shape(pred)[0]\n",
    "    # Step 2: Convert \"pred\" into an np.array() of indices with the maximum probabilities\n",
    "    indices = np.zeros((Ty,1))\n",
    "    for t in range(Ty):\n",
    "        distr = pred[t][0][:]\n",
    "        indices[t,0] = np.random.choice( range(n_values), p=distr)\n",
    "    # Step 3: Convert indices to one-hot vectors, the shape of the results should be (Ty, n_values)\n",
    "    results = to_categorical(indices, num_classes=x_initializer.shape[2])\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return results, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracks applicable seat door and key floor of tightly                     \n"
     ]
    }
   ],
   "source": [
    "results, indices = predict_and_sample2(inference_model2, x_initializer, a_initializer, c_initializer)\n",
    "\n",
    "# Transform the result to an actual sentence\n",
    "sample_sentence_words = [ix_to_word[int(index)] for index in list(indices)]\n",
    "sample_sentence = ' '.join(sample_sentence_words)\n",
    "sample_sentence = sample_sentence[0].upper() + sample_sentence[1:]\n",
    "print(sample_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Model 3: choose at random at each step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define a model that uses each step prediction as the input for next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_inference_model3(LSTM_cell, densor, Ty=30):\n",
    "    \"\"\"\n",
    "    Uses the trained \"LSTM_cell\" and \"densor\" from model() to generate a sequence of values.\n",
    "    \n",
    "    Arguments:\n",
    "    LSTM_cell -- the trained \"LSTM_cell\" from model(), Keras layer object\n",
    "    densor -- the trained \"densor\" from model(), Keras layer object\n",
    "    Ty -- integer, number of time steps to generate\n",
    "    \n",
    "    Returns:\n",
    "    inference_model -- Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the shape of input values\n",
    "    n_values = densor.units\n",
    "    # Get the number of the hidden state vector\n",
    "    n_a = LSTM_cell.units\n",
    "    \n",
    "    # Define the input of your model with a shape \n",
    "    x0 = Input(shape=(1, n_values))\n",
    "    \n",
    "    # Define s0, initial hidden state for the decoder LSTM\n",
    "    a0 = Input(shape=(n_a,), name='a0')\n",
    "    c0 = Input(shape=(n_a,), name='c0')\n",
    "    a = a0\n",
    "    c = c0\n",
    "    x = x0\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    # Step 1: Create an empty list of \"outputs\" to later store your predicted values (≈1 line)\n",
    "    output_indices = []\n",
    "    \n",
    "    # Step 2: Loop over Ty and generate a value at every time step\n",
    "    for t in range(Ty):\n",
    "        # Step 2.A: Perform one step of LSTM_cell. Use \"x\", not \"x0\" (≈1 line)\n",
    "        a, _, c = LSTM_cell(x, initial_state=[a,c])\n",
    "        \n",
    "        # Step 2.B: Apply Dense layer to the hidden state output of the LSTM_cell (≈1 line)\n",
    "        distr = densor(a)\n",
    "        # Step 2.C: Append the prediction \"out\" to \"outputs\". out.shape = (None, n_values) (≈1 line)\n",
    " \n",
    "        # Step 2.D: \n",
    "        # Select the next value according to \"out\",\n",
    "        # Set \"x\" to be the one-hot representation of the selected value\n",
    "        # See instructions above.\n",
    "        \n",
    "        x = tf.random.categorical(distr,1)\n",
    "        x = tf.squeeze(x, axis=-1)\n",
    "        #x = K.print_tensor(x, message='x = ')\n",
    "        output_indices.append(x)\n",
    "        \n",
    "        x = tf.one_hot(x,n_values)\n",
    "        # Step 2.E: \n",
    "        # Use RepeatVector(1) to convert x into a tensor with shape=(None, 1, n_values)\n",
    "        x = RepeatVector(1)(x)\n",
    "        \n",
    "    # Step 3: Create model instance with the correct \"inputs\" and \"outputs\" (≈1 line)\n",
    "    inference_model3 = Model(inputs=[x0,a0,c0], outputs=output_indices)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return inference_model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_model3 = sentence_inference_model3(LSTM_cell, densor, Ty = 30)\n",
    "inference_model3.set_weights(reconstructed_model.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use previous model to sample ('predict') a sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_initializer = np.zeros((1, 1, n_values))\n",
    "a_initializer = np.zeros((1, n_a))\n",
    "c_initializer = np.zeros((1, n_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_sample3(inference_model3, x_initializer = x_initializer, a_initializer = a_initializer, \n",
    "                       c_initializer = c_initializer):\n",
    "    \"\"\"\n",
    "    Predicts the next value of values using the inference model.\n",
    "    \n",
    "    Arguments:\n",
    "    inference_model -- Keras model instance for inference time\n",
    "    x_initializer -- numpy array of shape (1, 1, n_values), one-hot vector initializing the values generation\n",
    "    a_initializer -- numpy array of shape (1, n_a), initializing the hidden state of the LSTM_cell\n",
    "    c_initializer -- numpy array of shape (1, n_a), initializing the cell state of the LSTM_cel\n",
    "    \n",
    "    Returns:\n",
    "    results -- numpy-array of shape (Ty, 90), matrix of one-hot vectors representing the values generated\n",
    "    indices -- numpy-array of shape (Ty, 1), matrix of indices representing the values generated\n",
    "    \"\"\"\n",
    "    \n",
    "    n_values = x_initializer.shape[2]\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Step 1: Use your inference model to predict an output sequence given x_initializer, a_initializer and c_initializer.\n",
    "    predicted_output_indices = inference_model3.predict([x_initializer, a_initializer, c_initializer])\n",
    "    # Step 2: Convert indices to one-hot vectors, the shape of the results should be (Ty, n_values)\n",
    "    results = to_categorical(predicted_output_indices, num_classes=x_initializer.shape[2])\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return results, predicted_output_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 g04 2xze98 j39 211vm sides 69 gone future limitations manifest cleaner centerline spotlight inhibited beams grit resinae 3043vc fixture ip2 hammer responder customer barrel flare 4011wd2 indicator 5a 5900we\n"
     ]
    }
   ],
   "source": [
    "results, predicted_output_indices = predict_and_sample3(inference_model3, x_initializer, a_initializer, c_initializer)\n",
    "\n",
    "# Transform the result to an actual sentence\n",
    "sample_sentence_words = [ix_to_word[int(index)] for index in list(predicted_output_indices)]\n",
    "sample_sentence = ' '.join(sample_sentence_words)\n",
    "sample_sentence = sample_sentence[0].upper() + sample_sentence[1:]\n",
    "print(sample_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Model 4: provide 1st word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define a model that uses each step prediction as the input for next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_inference_model4(LSTM_cell, densor, Ty=30):\n",
    "    \"\"\"\n",
    "    Uses the trained \"LSTM_cell\" and \"densor\" from model() to generate a sequence of values.\n",
    "    \n",
    "    Arguments:\n",
    "    LSTM_cell -- the trained \"LSTM_cell\" from model(), Keras layer object\n",
    "    densor -- the trained \"densor\" from model(), Keras layer object\n",
    "    Ty -- integer, number of time steps to generate\n",
    "    \n",
    "    Returns:\n",
    "    inference_model -- Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the shape of input values\n",
    "    n_values = densor.units\n",
    "    # Get the number of the hidden state vector\n",
    "    n_a = LSTM_cell.units\n",
    "    \n",
    "    # Define the input of your model with a shape \n",
    "    x0 = Input(shape=(1, n_values))\n",
    "    x1 = Input(shape=(n_values))\n",
    "    \n",
    "    \n",
    "    # Define s0, initial hidden state for the decoder LSTM\n",
    "    a0 = Input(shape=(n_a,), name='a0')\n",
    "    c0 = Input(shape=(n_a,), name='c0')\n",
    "    a = a0\n",
    "    c = c0\n",
    "    x = x0\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    # Step 1: Create an empty list of \"outputs\" to later store your predicted values (≈1 line)\n",
    "    outputs = []\n",
    "    \n",
    "    # Step 2: Loop over Ty and generate a value at every time step\n",
    "    for t in range(Ty):\n",
    "        # Step 2.A: Perform one step of LSTM_cell. Use \"x\", not \"x0\" (≈1 line)\n",
    "        a, _, c = LSTM_cell(x, initial_state=[a,c])\n",
    "        \n",
    "        # Step 2.B: Apply Dense layer to the hidden state output of the LSTM_cell (≈1 line)\n",
    "        out = densor(a)\n",
    "        # Force first word to the one given\n",
    "        if t == 0:\n",
    "            out = tf.cast(x1, tf.int32)\n",
    "        # Step 2.C: Append the prediction \"out\" to \"outputs\". out.shape = (None, n_values) (≈1 line)\n",
    "        outputs.append(out)\n",
    " \n",
    "        # Step 2.D: \n",
    "        # Select the next value according to \"out\",\n",
    "        # Set \"x\" to be the one-hot representation of the selected value\n",
    "        # See instructions above.\n",
    "        x = tf.math.argmax(out, axis=-1)\n",
    "        x = tf.one_hot(x,n_values)\n",
    "        # Step 2.E: \n",
    "        # Use RepeatVector(1) to convert x into a tensor with shape=(None, 1, n_values)\n",
    "        x = RepeatVector(1)(x)\n",
    "        \n",
    "    # Step 3: Create model instance with the correct \"inputs\" and \"outputs\" (≈1 line)\n",
    "    inference_model4 = Model(inputs=[x0,x1,a0,c0], outputs=outputs)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return inference_model4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_model4 = sentence_inference_model4(LSTM_cell, densor, Ty = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use previous model to sample ('predict') a sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_initializer = np.zeros((1, 1, n_values))\n",
    "x1 = np.zeros((1, n_values))\n",
    "a_initializer = np.zeros((1, n_a))\n",
    "c_initializer = np.zeros((1, n_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_sample4(inference_model4, x_initializer = x_initializer, x1 = x1, a_initializer = a_initializer, \n",
    "                       c_initializer = c_initializer):\n",
    "    \"\"\"\n",
    "    Predicts the next value of values using the inference model.\n",
    "    \n",
    "    Arguments:\n",
    "    inference_model -- Keras model instance for inference time\n",
    "    x_initializer -- numpy array of shape (1, 1, n_values), one-hot vector initializing the values generation\n",
    "    a_initializer -- numpy array of shape (1, n_a), initializing the hidden state of the LSTM_cell\n",
    "    c_initializer -- numpy array of shape (1, n_a), initializing the cell state of the LSTM_cel\n",
    "    \n",
    "    Returns:\n",
    "    results -- numpy-array of shape (Ty, 90), matrix of one-hot vectors representing the values generated\n",
    "    indices -- numpy-array of shape (Ty, 1), matrix of indices representing the values generated\n",
    "    \"\"\"\n",
    "    \n",
    "    n_values = x_initializer.shape[2]\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Step 1: Use your inference model to predict an output sequence given x_initializer, a_initializer and c_initializer.\n",
    "    pred = inference_model4.predict([x_initializer, x1, a_initializer, c_initializer])\n",
    "    # Step 2: Convert \"pred\" into an np.array() of indices with the maximum probabilities\n",
    "    indices = np.argmax(pred, axis=-1)\n",
    "    # Step 3: Convert indices to one-hot vectors, the shape of the results should be (Ty, n_values)\n",
    "    results = to_categorical(indices, num_classes=x_initializer.shape[2])\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return results, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_initializer = np.zeros((1, 1, n_values))\n",
    "\n",
    "# Force first word\n",
    "init = np.zeros((1, n_values))\n",
    "init[0,word_to_ix['if']] = 1\n",
    "x1 = init\n",
    "\n",
    "a_initializer = np.zeros((1, n_a))\n",
    "c_initializer = np.zeros((1, n_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1366 attach\n"
     ]
    }
   ],
   "source": [
    "x_initializer = np.zeros((1, 1, n_values))\n",
    "\n",
    "# Take first word at random\n",
    "init = np.zeros((1, n_values))\n",
    "random_word_index = np.random.choice(np.arange(n_values), p=first_word_freq)\n",
    "print(random_word_index, ix_to_word[random_word_index])\n",
    "\n",
    "init[0,random_word_index] = 1\n",
    "x1 = init\n",
    "\n",
    "a_initializer = np.zeros((1, n_a))\n",
    "c_initializer = np.zeros((1, n_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attach the electrical connector part to the related electrical connector part <eos>                  \n"
     ]
    }
   ],
   "source": [
    "results, indices = predict_and_sample4(inference_model4, x_initializer, x1, a_initializer, c_initializer)\n",
    "\n",
    "#print(\"np.argmax(results[12]) =\", np.argmax(results[0]))\n",
    "#print(\"np.argmax(results[17]) =\", np.argmax(results[17]))\n",
    "#print(\"list(indices[12:18]) =\", list(indices[12:18]))\n",
    "\n",
    "# Transform the result to an actual sentence\n",
    "sample_sentence_words = [ix_to_word[int(index)] for index in list(indices)]\n",
    "sample_sentence = ' '.join(sample_sentence_words)\n",
    "sample_sentence = sample_sentence[0].upper() + sample_sentence[1:]\n",
    "print(sample_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Model 5: provide words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define a model that uses each step prediction as the input for next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_inference_model5(LSTM_cell, densor, Ty=30):\n",
    "    \"\"\"\n",
    "    Uses the trained \"LSTM_cell\" and \"densor\" from model() to generate a sequence of values.\n",
    "    \n",
    "    Arguments:\n",
    "    LSTM_cell -- the trained \"LSTM_cell\" from model(), Keras layer object\n",
    "    densor -- the trained \"densor\" from model(), Keras layer object\n",
    "    Ty -- integer, number of time steps to generate\n",
    "    \n",
    "    Returns:\n",
    "    inference_model -- Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the shape of input values\n",
    "    n_values = densor.units\n",
    "    # Get the number of the hidden state vector\n",
    "    n_a = LSTM_cell.units\n",
    "    \n",
    "    # Define the input of your model with a shape \n",
    "    x0 = Input(shape=(1, n_values))\n",
    "    x1 = Input(shape=(2, n_values))\n",
    "    given_words = np.zeros((2, n_values))\n",
    "    #for t in range(1):\n",
    "    #    given_words[t] = x1[t,:]\n",
    "    \n",
    "    \n",
    "    # Define s0, initial hidden state for the decoder LSTM\n",
    "    a0 = Input(shape=(n_a,), name='a0')\n",
    "    c0 = Input(shape=(n_a,), name='c0')\n",
    "    a = a0\n",
    "    c = c0\n",
    "    x = x0\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    # Step 1: Create an empty list of \"outputs\" to later store your predicted values (≈1 line)\n",
    "    outputs = []\n",
    "    \n",
    "    # Step 2: Loop over Ty and generate a value at every time step\n",
    "    for t in range(Ty):\n",
    "        # Step 2.A: Perform one step of LSTM_cell. Use \"x\", not \"x0\" (≈1 line)\n",
    "        a, _, c = LSTM_cell(x, initial_state=[a,c])\n",
    "        \n",
    "        # Step 2.B: Apply Dense layer to the hidden state output of the LSTM_cell (≈1 line)\n",
    "        out = densor(a)\n",
    "        # Force first word to the one given\n",
    "        if t <= 1:\n",
    "            out = tf.cast(x1[t,:], tf.int32)\n",
    "        # Step 2.C: Append the prediction \"out\" to \"outputs\". out.shape = (None, n_values) (≈1 line)\n",
    "        outputs.append(out)\n",
    " \n",
    "        # Step 2.D: \n",
    "        # Select the next value according to \"out\",\n",
    "        # Set \"x\" to be the one-hot representation of the selected value\n",
    "        # See instructions above.\n",
    "        x = tf.math.argmax(out, axis=-1)\n",
    "        x = tf.one_hot(x,n_values)\n",
    "        # Step 2.E: \n",
    "        # Use RepeatVector(1) to convert x into a tensor with shape=(None, 1, n_values)\n",
    "        x = RepeatVector(1)(x)\n",
    "        \n",
    "    # Step 3: Create model instance with the correct \"inputs\" and \"outputs\" (≈1 line)\n",
    "    inference_model5 = Model(inputs=[x0,x1,a0,c0], outputs=outputs)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return inference_model5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_model5 = sentence_inference_model5(LSTM_cell, densor, Ty = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use previous model to sample ('predict') a sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_initializer = np.zeros((1, 1, n_values))\n",
    "x1 = np.zeros((1, 2, n_values))\n",
    "a_initializer = np.zeros((1, n_a))\n",
    "c_initializer = np.zeros((1, n_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_sample5(inference_model5, x_initializer = x_initializer, x1 = x1, a_initializer = a_initializer, \n",
    "                       c_initializer = c_initializer):\n",
    "    \"\"\"\n",
    "    Predicts the next value of values using the inference model.\n",
    "    \n",
    "    Arguments:\n",
    "    inference_model -- Keras model instance for inference time\n",
    "    x_initializer -- numpy array of shape (1, 1, n_values), one-hot vector initializing the values generation\n",
    "    a_initializer -- numpy array of shape (1, n_a), initializing the hidden state of the LSTM_cell\n",
    "    c_initializer -- numpy array of shape (1, n_a), initializing the cell state of the LSTM_cel\n",
    "    \n",
    "    Returns:\n",
    "    results -- numpy-array of shape (Ty, 90), matrix of one-hot vectors representing the values generated\n",
    "    indices -- numpy-array of shape (Ty, 1), matrix of indices representing the values generated\n",
    "    \"\"\"\n",
    "    \n",
    "    n_values = x_initializer.shape[2]\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Step 1: Use your inference model to predict an output sequence given x_initializer, a_initializer and c_initializer.\n",
    "    pred = inference_model5.predict([x_initializer, x1, a_initializer, c_initializer])\n",
    "    # Step 2: Convert \"pred\" into an np.array() of indices with the maximum probabilities\n",
    "    indices = np.argmax(pred, axis=-1)\n",
    "    # Step 3: Convert indices to one-hot vectors, the shape of the results should be (Ty, n_values)\n",
    "    results = to_categorical(indices, num_classes=x_initializer.shape[2])\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return results, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_initializer = np.zeros((1, 1, n_values))\n",
    "\n",
    "# Force first word\n",
    "init = np.zeros((1, 2, n_values))\n",
    "init[0,0,word_to_ix['if']] = 1\n",
    "init[0,1,word_to_ix['there']] = 1\n",
    "x1 = init\n",
    "\n",
    "a_initializer = np.zeros((1, n_a))\n",
    "c_initializer = np.zeros((1, n_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'model_24/tf.__operators__.getitem_249/strided_slice' defined at (most recent call last):\n    File \"C:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n      \"__main__\", mod_spec)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n      exec(code, run_globals)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 664, in launch_instance\n      app.start()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 563, in start\n      self.io_loop.start()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 148, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 534, in run_forever\n      self._run_once()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1771, in _run_once\n      handle._run()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\events.py\", line 88, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 690, in <lambda>\n      lambda f: self._run_callback(functools.partial(callback, future))\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 743, in _run_callback\n      ret = callback()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 787, in inner\n      self.run()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 748, in run\n      yielded = self.gen.send(value)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 365, in process_one\n      yield gen.maybe_future(dispatch(*args))\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n      yielded = next(result)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 272, in dispatch_shell\n      yield gen.maybe_future(handler(stream, idents, msg))\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n      yielded = next(result)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 542, in execute_request\n      user_expressions, allow_stdin,\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n      yielded = next(result)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 294, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2855, in run_cell\n      raw_cell, store_history, silent, shell_futures)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in _run_cell\n      return runner(coro)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3058, in run_cell_async\n      interactivity=interactivity, compiler=compiler, result=result)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3249, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3326, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"<ipython-input-313-416b38e84fd0>\", line 1, in <module>\n      results, indices = predict_and_sample5(inference_model5, x_initializer, x1, a_initializer, c_initializer)\n    File \"<ipython-input-312-f0ca37dd0bfa>\", line 21, in predict_and_sample5\n      pred = inference_model5.predict([x_initializer, x1, a_initializer, c_initializer])\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1982, in predict\n      tmp_batch_outputs = self.predict_function(iterator)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1801, in predict_function\n      return step_function(self, iterator)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1790, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1783, in run_step\n      outputs = model.predict_step(data)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1751, in predict_step\n      return self(x, training=False)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\functional.py\", line 452, in call\n      inputs, training=training, mask=mask)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\functional.py\", line 589, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\layers\\core\\tf_op_layer.py\", line 503, in _call_wrapper\n      return original_call(*new_args, **new_kwargs)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\layers\\core\\tf_op_layer.py\", line 226, in _call_wrapper\n      return self._call_wrapper(*args, **kwargs)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\layers\\core\\tf_op_layer.py\", line 261, in _call_wrapper\n      result = self.function(*args, **kwargs)\nNode: 'model_24/tf.__operators__.getitem_249/strided_slice'\nslice index 1 of dimension 0 out of bounds.\n\t [[{{node model_24/tf.__operators__.getitem_249/strided_slice}}]] [Op:__inference_predict_function_703904]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-313-416b38e84fd0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mresults\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_and_sample5\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minference_model5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_initializer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_initializer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_initializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#print(\"np.argmax(results[12]) =\", np.argmax(results[0]))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#print(\"np.argmax(results[17]) =\", np.argmax(results[17]))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#print(\"list(indices[12:18]) =\", list(indices[12:18]))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-312-f0ca37dd0bfa>\u001b[0m in \u001b[0;36mpredict_and_sample5\u001b[1;34m(inference_model5, x_initializer, x1, a_initializer, c_initializer)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;31m### START CODE HERE ###\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;31m# Step 1: Use your inference model to predict an output sequence given x_initializer, a_initializer and c_initializer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minference_model5\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx_initializer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_initializer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_initializer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[1;31m# Step 2: Convert \"pred\" into an np.array() of indices with the maximum probabilities\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 55\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'model_24/tf.__operators__.getitem_249/strided_slice' defined at (most recent call last):\n    File \"C:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n      \"__main__\", mod_spec)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n      exec(code, run_globals)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 664, in launch_instance\n      app.start()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 563, in start\n      self.io_loop.start()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 148, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 534, in run_forever\n      self._run_once()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1771, in _run_once\n      handle._run()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\events.py\", line 88, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 690, in <lambda>\n      lambda f: self._run_callback(functools.partial(callback, future))\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 743, in _run_callback\n      ret = callback()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 787, in inner\n      self.run()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 748, in run\n      yielded = self.gen.send(value)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 365, in process_one\n      yield gen.maybe_future(dispatch(*args))\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n      yielded = next(result)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 272, in dispatch_shell\n      yield gen.maybe_future(handler(stream, idents, msg))\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n      yielded = next(result)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 542, in execute_request\n      user_expressions, allow_stdin,\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n      yielded = next(result)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 294, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2855, in run_cell\n      raw_cell, store_history, silent, shell_futures)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in _run_cell\n      return runner(coro)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3058, in run_cell_async\n      interactivity=interactivity, compiler=compiler, result=result)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3249, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3326, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"<ipython-input-313-416b38e84fd0>\", line 1, in <module>\n      results, indices = predict_and_sample5(inference_model5, x_initializer, x1, a_initializer, c_initializer)\n    File \"<ipython-input-312-f0ca37dd0bfa>\", line 21, in predict_and_sample5\n      pred = inference_model5.predict([x_initializer, x1, a_initializer, c_initializer])\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1982, in predict\n      tmp_batch_outputs = self.predict_function(iterator)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1801, in predict_function\n      return step_function(self, iterator)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1790, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1783, in run_step\n      outputs = model.predict_step(data)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1751, in predict_step\n      return self(x, training=False)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\functional.py\", line 452, in call\n      inputs, training=training, mask=mask)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\functional.py\", line 589, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\layers\\core\\tf_op_layer.py\", line 503, in _call_wrapper\n      return original_call(*new_args, **new_kwargs)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\layers\\core\\tf_op_layer.py\", line 226, in _call_wrapper\n      return self._call_wrapper(*args, **kwargs)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\layers\\core\\tf_op_layer.py\", line 261, in _call_wrapper\n      result = self.function(*args, **kwargs)\nNode: 'model_24/tf.__operators__.getitem_249/strided_slice'\nslice index 1 of dimension 0 out of bounds.\n\t [[{{node model_24/tf.__operators__.getitem_249/strided_slice}}]] [Op:__inference_predict_function_703904]"
     ]
    }
   ],
   "source": [
    "results, indices = predict_and_sample5(inference_model5, x_initializer, x1, a_initializer, c_initializer)\n",
    "\n",
    "#print(\"np.argmax(results[12]) =\", np.argmax(results[0]))\n",
    "#print(\"np.argmax(results[17]) =\", np.argmax(results[17]))\n",
    "#print(\"list(indices[12:18]) =\", list(indices[12:18]))\n",
    "\n",
    "# Transform the result to an actual sentence\n",
    "sample_sentence_words = [ix_to_word[int(index)] for index in list(indices)]\n",
    "sample_sentence = ' '.join(sample_sentence_words)\n",
    "sample_sentence = sample_sentence[0].upper() + sample_sentence[1:]\n",
    "print(sample_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
