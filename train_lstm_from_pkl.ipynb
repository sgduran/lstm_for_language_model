{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import pprint\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import Callback, CSVLogger\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Input, LSTM, Reshape, Lambda, RepeatVector\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset (1): build vocab, get frequency of words and length of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sentences_with_eos_and_unk.pkl', 'rb') as f:\n",
    "    sentences, counter = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7274 unique words in our dataset.\n",
      "There are 3380789 total words in our dataset.\n"
     ]
    }
   ],
   "source": [
    "# Build vocab\n",
    "vocab = sorted(counter.keys())\n",
    "vocab_size = len(vocab)\n",
    "print('There are %d unique words in our dataset.' % vocab_size)\n",
    "num_of_words = sum(counter.values())\n",
    "print('There are %d total words in our dataset.' % num_of_words)\n",
    "\n",
    "# Hash table for words to indices and viceversa\n",
    "# I don't use index 0 because it's used for padding the sequences\n",
    "word_to_ix = { w:i+1 for i,w in enumerate(vocab) }\n",
    "word_to_ix['<blank>'] = 0\n",
    "ix_to_word = { i+1:w for i,w in enumerate(vocab) }\n",
    "ix_to_word[0] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get frequency of words\n",
    "word_freq = np.zeros(vocab_size+1)\n",
    "norm_constant = num_of_words\n",
    "for i,key in enumerate(vocab):\n",
    "    word_freq[i+1] = counter[key] / norm_constant\n",
    "    \n",
    "# Get frequency of first words\n",
    "first_word_freq = np.zeros(vocab_size+1)\n",
    "norm_constant = len(sentences)\n",
    "for i,sentence in enumerate(sentences):\n",
    "    first_word_freq[ word_to_ix[sentence.lower().split()[0]] ] += 1\n",
    "first_word_freq /= len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.58 % of the sentences have more than 30 words.\n"
     ]
    }
   ],
   "source": [
    "# Short analysis of length of sentences\n",
    "n = 30\n",
    "count_less_than_n_words = 0\n",
    "maxLen = 0\n",
    "counter_length = Counter([])\n",
    "for sentence in sentences:\n",
    "    counter_length += Counter([len(sentence.lower().split())])\n",
    "    maxLen = max(maxLen,len(sentence.lower().split()))\n",
    "    if len(sentence.lower().split()) >= n:\n",
    "        count_less_than_n_words += 1\n",
    "\n",
    "proportion =  (count_less_than_n_words / len(sentences)) * 100\n",
    "        \n",
    "print('%.2f %% of the sentences have more than %d words.' % (proportion,n)  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset (2): split train-trest, one-hot vectors, generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_and_split_training_set(sentences_original, percentage_test=2):\n",
    "    # shuffle at unison\n",
    "    print('Shuffling sentences')\n",
    "\n",
    "    tmp_sentences = []\n",
    "    for i in np.random.RandomState(seed=0).permutation(len(sentences_original)):\n",
    "        tmp_sentences.append(sentences_original[i])\n",
    "\n",
    "    cut_index = int(len(sentences_original) * (1.-(percentage_test/100.)))\n",
    "    x_train, x_test = tmp_sentences[:cut_index], tmp_sentences[cut_index:]\n",
    "\n",
    "    print(\"Size of training set = %d\" % len(x_train))\n",
    "    print(\"Size of test set = %d\" % len(x_test))\n",
    "    return x_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling sentences\n",
      "Size of training set = 221835\n",
      "Size of test set = 4528\n"
     ]
    }
   ],
   "source": [
    "sentences_train, sentences_test = shuffle_and_split_training_set(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Put the clips part  part and part of the loom knoop hardness test value   channel B part  the loom HB2   channel B part and the No 2 starboard igniter lead part with the bolt part and the spacers part and part in position on the oil tube part and loosely install the nut part  <eos>',\n",
       " 'Put the clips part  part  part and part of the air tube part  the fuel tube part  the fuel tube part and the loom database part with the spacers part  part and part and the bolt part in position on the nut part attached to the bracket part  and loosely install the bolt part  <eos>',\n",
       " 'Make sure that the ignition lever part of the door damper part and the emergency opening actuator part is in the ARMED position  <eos>',\n",
       " 'You can use these pressurization sources as an alternative  or   <eos>',\n",
       " 'Disconnect the electrical connectors from the 115VAC EPDC2 temporary revision 1422XZ and the 230VAC EPDC2 temporary revision 1424XZ in module 2H   <eos>',\n",
       " 'Alodine 1500 mil <unk> <unk> <eos>',\n",
       " 'The transducer part <eos>',\n",
       " 'Remove the nuts part from the bolts part  <eos>',\n",
       " 'For the steps that follow the POLISHING KIT   MICRO MESH is necessary  <eos>',\n",
       " 'Loosen the nuts part of all the support struts part  <eos>',\n",
       " 'The Yellow left outboard aileron servocontrol  <eos>',\n",
       " 'The applicable lavatory occupied signs are removed   <eos>',\n",
       " 'Put the seals part in position on the left ottoman section part  the right ottoman section part and the upper coat stowage section part  <eos>',\n",
       " 'In the Status column wait for the status of the related content inventory list to show Load Complete   <eos>',\n",
       " 'Remove the nut part and the bolt part attached to the tube part  <eos>',\n",
       " 'Make sure that flap torque shaft assembly 5027CV part is removed   <eos>',\n",
       " 'It is possible to restore the avionics server function cabinet with the group 1 and amperes per square foot software components    only and then dispatch the aircraft under Master Minimum Equipment List MMEL condition <eos>',\n",
       " 'Bulk cargo compartment  <eos>',\n",
       " 'Re prime the center tank pumps   <eos>',\n",
       " 'If there is galling accept the upper interservices fairing  <eos>']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_test[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When transforming to indices, we clean up: we use .lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_indices(X, word_to_ix, max_len):\n",
    "    \"\"\"\n",
    "    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.\n",
    "    The output shape should be such that it can be given to `Embedding()` (described in Figure 4). \n",
    "    \n",
    "    Arguments:\n",
    "    X -- array of sentences (strings), of shape (m, 1)\n",
    "    word_to_index -- a dictionary containing the each word mapped to its index\n",
    "    max_len -- maximum number of words in a sentence. You can assume every sentence in X is no longer than this. \n",
    "    \n",
    "    Returns:\n",
    "    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[0]                                   # number of training examples\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Initialize X_indices as a numpy matrix of zeros and the correct shape (≈ 1 line)\n",
    "    X_indices = np.zeros( (m, max_len) )\n",
    "    \n",
    "    for i in range(m):                               # loop over training examples\n",
    "        \n",
    "        # Convert the ith training sentence in lower case and split is into words. You should get a list of words.\n",
    "        sentence_words = X[i].lower().split()\n",
    "        # We truncate sentence_words at max_len\n",
    "        sentence_words = sentence_words[:max_len]\n",
    "        \n",
    "        # Initialize j to 0\n",
    "        j = 0\n",
    "        \n",
    "        # Loop over the words of sentence_words\n",
    "\n",
    "        for w in sentence_words:\n",
    "            # if w exists in the word_to_index dictionary\n",
    "            if w in word_to_ix:\n",
    "                # Set the (i,j)th entry of X_indices to the index of the correct word.\n",
    "                X_indices[i, j] = word_to_ix[w]\n",
    "                # Increment j to j + 1\n",
    "                j += 1\n",
    "            \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return X_indices\n",
    "\n",
    "def convert_to_one_hot(X_indices, C):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)\n",
    "    C -- size of one-hot vectors -1 place for 1 word in vocabulary, plus blank space-\n",
    "    \n",
    "    Returns:\n",
    "    X_oh -- array of one-hot vectors corresponding to words in the sentences from X, of shape (m, max_len, C)\n",
    "    \"\"\"\n",
    "    X_oh = np.eye(C)[X_indices]\n",
    "    return X_oh\n",
    "\n",
    "def generator(sentence_list, batch_size):\n",
    "    index = 0\n",
    "    while True:\n",
    "        X_oh = np.zeros((batch_size, Tx, len(vocab)+1))\n",
    "        Y_oh = np.zeros((batch_size, Tx, len(vocab)+1))\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            \n",
    "            # One sentences at a time\n",
    "            x_indices = sentences_to_indices(np.asarray(sentence_list[index:index+1]), word_to_ix, Tx)\n",
    "            X_oh[i,:,:] = convert_to_one_hot(x_indices.astype(int), C = len(vocab)+1)\n",
    "        \n",
    "            y_indices = np.zeros(np.shape(x_indices))\n",
    "            y_indices[:,0:Tx-1] = x_indices[:,1:Tx]\n",
    "            Y_oh[i,:,:] = convert_to_one_hot(y_indices.astype(int), C = len(vocab)+1)\n",
    "            \n",
    "            #for t, w in enumerate(sentence_list[index]):\n",
    "            #    x[i, t, word_indices[w]] = 1\n",
    "            #y[i, word_indices[next_word_list[index]]] = 1\n",
    "\n",
    "            index = index + 1\n",
    "            if index == len(sentence_list):\n",
    "                index = 0\n",
    "                \n",
    "        Y_oh = np.transpose(Y_oh, (1, 0, 2))\n",
    "        \n",
    "        a0 = np.zeros((batch_size, n_a))\n",
    "        c0 = np.zeros((batch_size, n_a))\n",
    "        model_input = [X_oh, a0, c0]\n",
    "        #print(np.shape(model_input))\n",
    "        #print(np.shape(X_oh), np.shape(a0), np.shape(c0), np.shape(Y_oh))\n",
    "        \n",
    "        yield model_input, list(Y_oh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model, optimizer, loss and metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequence generation uses a for-loop\n",
    "* If we're building an RNN where, at test time, the entire input sequence $x^{\\langle 1 \\rangle}, x^{\\langle 2 \\rangle}, \\ldots, x^{\\langle T_x \\rangle}$ is given in advance, then Keras has simple built-in functions to build the model. \n",
    "* However, for **sequence generation, at test time we won't know all the values of $x^{\\langle t\\rangle}$ in advance**.\n",
    "* Instead, we'll generate them one at a time using $x^{\\langle t\\rangle} = y^{\\langle t-1 \\rangle}$. \n",
    "    * The input at time \"t\" is the prediction at the previous time step \"t-1\".\n",
    "* So we'll need to implement our own for-loop to iterate over the time steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "n_a = 128 # number of dimensions for the hidden state of each LSTM cell.\n",
    "n_values = len(vocab) + 1 # Dimension of inputs one-hot encoded: number of words and the blank space.\n",
    "Tx = 30 # max len of sequences, already defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define layers\n",
    "reshaper = Reshape((1, n_values))                  # Used in Step 2.B of model(), below\n",
    "LSTM_cell = LSTM(n_a, return_state = True)         # Used in Step 2.C\n",
    "densor = Dense(n_values, activation='softmax')     # Used in Step 2.D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_model(Tx, LSTM_cell, densor, reshaper):\n",
    "    \"\"\"\n",
    "    Implement the model composed of Tx LSTM cells where each cell is responsible\n",
    "    for learning the following word based on the previous word and context.\n",
    "    Each cell has the following schema: \n",
    "            [X_{t}, a_{t-1}, c0_{t-1}] -> RESHAPE() -> LSTM() -> DENSE()\n",
    "    Arguments:\n",
    "        Tx -- length of the sequences in the corpus\n",
    "        LSTM_cell -- LSTM layer instance\n",
    "        densor -- Dense layer instance\n",
    "        reshaper -- Reshape layer instance\n",
    "    \n",
    "    Returns:\n",
    "        model -- a keras instance model with inputs [X, a0, c0]\n",
    "    \"\"\"\n",
    "    # Get the shape of input values\n",
    "    n_values = densor.units\n",
    "    \n",
    "    # Get the number of the hidden state vector\n",
    "    n_a = LSTM_cell.units\n",
    "    \n",
    "    # Define the input layer and specify the shape\n",
    "    X = Input(shape=(Tx, n_values)) \n",
    "    \n",
    "    # Define the initial hidden state a0 and initial cell state c0\n",
    "    # using `Input`\n",
    "    \n",
    "    a0 = Input(shape=(n_a,), name='a0')\n",
    "    c0 = Input(shape=(n_a,), name='c0')\n",
    "    a = a0\n",
    "    c = c0\n",
    "    ### START CODE HERE ### \n",
    "    # Step 1: Create empty list to append the outputs while you iterate (≈1 line)\n",
    "    outputs = []\n",
    "    \n",
    "    # Step 2: Loop over tx\n",
    "    for t in range(Tx):\n",
    "        \n",
    "        # Step 2.A: select the \"t\"th time step vector from X. \n",
    "        x = X[:,t,:]\n",
    "        # Step 2.B: Use reshaper to reshape x to be (1, n_values) (≈1 line)\n",
    "        x = reshaper(x)\n",
    "        # Step 2.C: Perform one step of the LSTM_cell\n",
    "        a, _, c = LSTM_cell(inputs=x, initial_state=[a, c])\n",
    "        # Step 2.D: Apply densor to the hidden state output of LSTM_Cell\n",
    "        out = densor(a)\n",
    "        # Step 2.E: add the output to \"outputs\"\n",
    "        outputs.append(out)\n",
    "        \n",
    "    # Step 3: Create model instance\n",
    "    model = Model(inputs=[X, a0, c0], outputs=outputs)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = loop_model(Tx=30, LSTM_cell=LSTM_cell, densor=densor, reshaper=reshaper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "opt = Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, decay=0.01)\n",
    "\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inference_model1.predict([x_initializer, a_initializer, c_initializer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "\n",
    "class LossHistoryPerBatch(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "\n",
    "# Does not work very well\n",
    "class NBatchLogger(Callback):\n",
    "    def __init__(self,display=50):\n",
    "        '''\n",
    "        display: Number of batches to wait before outputting loss\n",
    "        '''\n",
    "        self.seen = 0\n",
    "        self.display = display\n",
    "        \n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        # Logs does not have size key. Then 'seen' variable is not useful\n",
    "        self.seen += logs.get('size', 0)\n",
    "        print('\\n', logs.get('size',0), self.seen, self.display)\n",
    "        if self.seen % self.display == 0:\n",
    "            print('\\n {}/{} - Batch loss: {} \\n'.format(self.seen,self.params['steps'],logs.get('loss')) )\n",
    "\n",
    "csv_logger = CSVLogger('log.csv', append=True, separator=';')\n",
    "\n",
    "out_batch = LossHistoryPerBatch()\n",
    "\n",
    "checkpoint_filepath = './tmp/checkpoint'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=False,\n",
    "    monitor='val_loss',\n",
    "    mode='max',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-24b137e0cc44>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m history = model.fit( generator(sentences_train, batch_size = BATCH_SIZE), \n\u001b[0;32m      4\u001b[0m                     \u001b[0msteps_per_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                     \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "t = time() \n",
    "\n",
    "history = model.fit( generator(sentences_train, batch_size = BATCH_SIZE), \n",
    "                    steps_per_epoch = int(len(sentences_train)/BATCH_SIZE) + 1,\n",
    "                    epochs = EPOCHS,\n",
    "                    validation_data = generator(sentences_test, batch_size = BATCH_SIZE),\n",
    "                    validation_steps = int(len(sentences_test)/BATCH_SIZE) + 1,\n",
    "                    callbacks = [model_checkpoint_callback, csv_logger, out_batch],\n",
    "                    verbose = 1)\n",
    "\n",
    "print('\\n Time for training the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: 500k sentences, for 100 epochs\n",
    "# With Mini batch of 64:\n",
    "# 0.37 mins for 1 epoch for 500 --> 25 days\n",
    "# 2.98 mins for 1 epoch for 5k --> 21 days\n",
    "# 35.83 mins for 5 epochs for 10k\n",
    "\n",
    "# With Mini batch of 128:\n",
    "# 0.39 mins for 1 epoch for 500 --> 25 days\n",
    "# 3.15mins mins for 1 epoch for 5k --> 22 days\n",
    "\n",
    "# With Mini batch of 256: Out of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at epoch 1: 0.41285210847854614\n",
      "loss at epoch 5: 0.4438820481300354\n",
      "loss at epoch 10: 0.453125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x23b8d244208>]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot loss\n",
    "print(f\"loss at epoch 1: {history.history['val_dense_2_accuracy'][0]}\")\n",
    "print(f\"loss at epoch 5: {history.history['val_dense_2_accuracy'][EPOCHS-2]}\")\n",
    "print(f\"loss at epoch 10: {history.history['val_dense_2_accuracy'][EPOCHS-1]}\")\n",
    "plt.plot(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: trained_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: trained_model\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x0000023B53A599C8> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    }
   ],
   "source": [
    "# Save model and weights\n",
    "model.save('trained_model')\n",
    "model.save_weights(\"trained_weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./trained_model/trainHistoryDict.pkl', 'wb') as file_pi:\n",
    "    pickle.dump(history.history, file_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Model 2: force 1st word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define a model that uses each step prediction as the input for next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_inference_model2(LSTM_cell, densor, Ty=30):\n",
    "    \"\"\"\n",
    "    Uses the trained \"LSTM_cell\" and \"densor\" from model() to generate a sequence of values.\n",
    "    \n",
    "    Arguments:\n",
    "    LSTM_cell -- the trained \"LSTM_cell\" from model(), Keras layer object\n",
    "    densor -- the trained \"densor\" from model(), Keras layer object\n",
    "    Ty -- integer, number of time steps to generate\n",
    "    \n",
    "    Returns:\n",
    "    inference_model -- Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the shape of input values\n",
    "    n_values = densor.units\n",
    "    # Get the number of the hidden state vector\n",
    "    n_a = LSTM_cell.units\n",
    "    \n",
    "    # Define the input of your model with a shape \n",
    "    x0 = Input(shape=(1, n_values))\n",
    "    x1 = Input(shape=(n_values))\n",
    "    \n",
    "    \n",
    "    # Define s0, initial hidden state for the decoder LSTM\n",
    "    a0 = Input(shape=(n_a,), name='a0')\n",
    "    c0 = Input(shape=(n_a,), name='c0')\n",
    "    a = a0\n",
    "    c = c0\n",
    "    x = x0\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    # Step 1: Create an empty list of \"outputs\" to later store your predicted values (≈1 line)\n",
    "    outputs = []\n",
    "    \n",
    "    # Step 2: Loop over Ty and generate a value at every time step\n",
    "    for t in range(Ty):\n",
    "        # Step 2.A: Perform one step of LSTM_cell. Use \"x\", not \"x0\" (≈1 line)\n",
    "        a, _, c = LSTM_cell(x, initial_state=[a,c])\n",
    "        \n",
    "        # Step 2.B: Apply Dense layer to the hidden state output of the LSTM_cell (≈1 line)\n",
    "        out = densor(a)\n",
    "        # Force first word to the one given\n",
    "        if t == 0:\n",
    "            out = tf.cast(x1, tf.int32)\n",
    "        # Step 2.C: Append the prediction \"out\" to \"outputs\". out.shape = (None, n_values) (≈1 line)\n",
    "        outputs.append(out)\n",
    " \n",
    "        # Step 2.D: \n",
    "        # Select the next value according to \"out\",\n",
    "        # Set \"x\" to be the one-hot representation of the selected value\n",
    "        # See instructions above.\n",
    "        x = tf.math.argmax(out, axis=-1)\n",
    "        x = tf.one_hot(x,n_values)\n",
    "        # Step 2.E: \n",
    "        # Use RepeatVector(1) to convert x into a tensor with shape=(None, 1, n_values)\n",
    "        x = RepeatVector(1)(x)\n",
    "        \n",
    "    # Step 3: Create model instance with the correct \"inputs\" and \"outputs\" (≈1 line)\n",
    "    inference_model = Model(inputs=[x0,x1,a0,c0], outputs=outputs)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return inference_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_model2 = sentence_inference_model2(LSTM_cell, densor, Ty = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use previous model to sample ('predict') a sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_initializer = np.zeros((1, 1, n_values))\n",
    "x1 = np.zeros((1, n_values))\n",
    "a_initializer = np.zeros((1, n_a))\n",
    "c_initializer = np.zeros((1, n_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_sample2(inference_model2, x_initializer = x_initializer, x1 = x1, a_initializer = a_initializer, \n",
    "                       c_initializer = c_initializer):\n",
    "    \"\"\"\n",
    "    Predicts the next value of values using the inference model.\n",
    "    \n",
    "    Arguments:\n",
    "    inference_model -- Keras model instance for inference time\n",
    "    x_initializer -- numpy array of shape (1, 1, n_values), one-hot vector initializing the values generation\n",
    "    a_initializer -- numpy array of shape (1, n_a), initializing the hidden state of the LSTM_cell\n",
    "    c_initializer -- numpy array of shape (1, n_a), initializing the cell state of the LSTM_cel\n",
    "    \n",
    "    Returns:\n",
    "    results -- numpy-array of shape (Ty, 90), matrix of one-hot vectors representing the values generated\n",
    "    indices -- numpy-array of shape (Ty, 1), matrix of indices representing the values generated\n",
    "    \"\"\"\n",
    "    \n",
    "    n_values = x_initializer.shape[2]\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Step 1: Use your inference model to predict an output sequence given x_initializer, a_initializer and c_initializer.\n",
    "    pred = inference_model2.predict([x_initializer, x1, a_initializer, c_initializer])\n",
    "    # Step 2: Convert \"pred\" into an np.array() of indices with the maximum probabilities\n",
    "    indices = np.argmax(pred, axis=-1)\n",
    "    # Step 3: Convert indices to one-hot vectors, the shape of the results should be (Ty, n_values)\n",
    "    results = to_categorical(indices, num_classes=x_initializer.shape[2])\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return results, indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample using model 2 (from a given word or taken at random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_initializer = np.zeros((1, 1, n_values))\n",
    "\n",
    "# Force first word\n",
    "init = np.zeros((1, n_values))\n",
    "init[0,word_to_ix['remove']] = 1\n",
    "x1 = init\n",
    "\n",
    "a_initializer = np.zeros((1, n_a))\n",
    "c_initializer = np.zeros((1, n_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1112 adjust\n"
     ]
    }
   ],
   "source": [
    "x_initializer = np.zeros((1, 1, n_values))\n",
    "\n",
    "# Take first word at random\n",
    "init = np.zeros((1, n_values))\n",
    "random_word_index = np.random.choice(np.arange(n_values), p=first_word_freq)\n",
    "print(random_word_index, ix_to_word[random_word_index])\n",
    "\n",
    "init[0,random_word_index] = 1\n",
    "x1 = init\n",
    "\n",
    "a_initializer = np.zeros((1, n_a))\n",
    "c_initializer = np.zeros((1, n_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjust the applicable access panel part <eos>                       \n"
     ]
    }
   ],
   "source": [
    "results, indices = predict_and_sample2(inference_model2, x_initializer, x1, a_initializer, c_initializer)\n",
    "\n",
    "#print(\"np.argmax(results[12]) =\", np.argmax(results[0]))\n",
    "#print(\"np.argmax(results[17]) =\", np.argmax(results[17]))\n",
    "#print(\"list(indices[12:18]) =\", list(indices[12:18]))\n",
    "\n",
    "# Transform the result to an actual sentence\n",
    "sample_sentence_words = [ix_to_word[int(index)] for index in list(indices)]\n",
    "sample_sentence = ' '.join(sample_sentence_words)\n",
    "sample_sentence = sample_sentence[0].upper() + sample_sentence[1:]\n",
    "print(sample_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
